{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from mlbt.utils import PurgedKFold\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "\n",
    "def feat_importance(\n",
    "    events,\n",
    "    X,\n",
    "    y,\n",
    "    n_estimators=1000,\n",
    "    cv=10,\n",
    "    max_samples=1.0,\n",
    "    pct_embargo=0,\n",
    "    scoring=\"accuracy\",\n",
    "    method=\"MDI\",\n",
    "    min_w_leaf=0.0,\n",
    "    **kwargs\n",
    "):\n",
    "    logging.info(f\"feat_importance for {len(X.columns)} features\")\n",
    "    # 1) prepare classifier, cv. max_features=1 to prevent masking\n",
    "    # feature importance from a random forest\n",
    "    clf = DecisionTreeClassifier(\n",
    "        criterion=\"entropy\",\n",
    "        max_features=1,\n",
    "        class_weight=\"balanced\",\n",
    "        min_weight_fraction_leaf=min_w_leaf,\n",
    "    )\n",
    "\n",
    "    clf = BaggingClassifier(\n",
    "        base_estimator=clf,\n",
    "        n_estimators=n_estimators,\n",
    "        max_features=1.0,\n",
    "        max_samples=max_samples,\n",
    "        oob_score=True,\n",
    "    )\n",
    "\n",
    "    if method == \"MDI\":\n",
    "        fit = clf.fit(X=X, y=y)\n",
    "        imp = feat_imp_MDI(fit, feat_names=X.columns)\n",
    "    elif method == \"MDA\":\n",
    "        sample_weight = pd.Series(1, index=events.index)\n",
    "        imp = feat_imp_MDA(\n",
    "            clf=clf,\n",
    "            X=X,\n",
    "            y=y,\n",
    "            cv=cv,\n",
    "            sample_weight=sample_weight,\n",
    "            t1=events[\"t1\"],\n",
    "            pct_embargo=pct_embargo,\n",
    "            scoring=scoring,\n",
    "        )\n",
    "\n",
    "    imp = imp.sort_values(\"mean\", ascending=True)\n",
    "\n",
    "    return imp\n",
    "\n",
    "\n",
    "def feat_imp_MDI(fit, feat_names):\n",
    "    # feat importance based on IS mean impurity reduction\n",
    "    df0 = {i: tree.feature_importances_ for i, tree in enumerate(fit.estimators_)}\n",
    "    df0 = pd.DataFrame.from_dict(df0, orient=\"index\")\n",
    "    df0.columns = feat_names\n",
    "    df0 = df0.replace(0, np.nan)  # because max_features = 1\n",
    "    imp = pd.concat(\n",
    "        {\"mean\": df0.mean(), \"std\": df0.std() * df0.shape[0] ** -0.5}, axis=1\n",
    "    )\n",
    "    imp /= imp[\"mean\"].sum()\n",
    "    return imp\n",
    "\n",
    "\n",
    "def feat_imp_MDA(clf, X, y, cv, sample_weight, t1, pct_embargo, scoring=\"neg_log_loss\"):\n",
    "    # feat importance based on OOS score reduction\n",
    "    if scoring not in [\"neg_log_loss\", \"accuracy\"]:\n",
    "        raise ValueError(\"wrong scoring method\")\n",
    "    from sklearn.metrics import log_loss, accuracy_score\n",
    "    logging.debug(f\"MDA with {cv}-fold CV\")\n",
    "\n",
    "    cv_gen = PurgedKFold(n_splits=cv, t1=t1, pct_embargo=pct_embargo)\n",
    "    scr0, scr1 = pd.Series(), pd.DataFrame(columns=X.columns)\n",
    "    for i, (train, test) in enumerate(cv_gen.split(X=X)):\n",
    "        X0, y0, w0 = X.iloc[train, :], y.iloc[train], sample_weight.iloc[train]\n",
    "        X1, y1, w1 = X.iloc[test, :], y.iloc[test], sample_weight.iloc[test]\n",
    "        fit = clf.fit(X=X0, y=y0, sample_weight=w0.values)\n",
    "        if scoring == \"neg_log_loss\":\n",
    "            prob = fit.predict_proba(X1)\n",
    "            scr0.loc[i] = -log_loss(\n",
    "                y1, prob, sample_weight=w1.values, labels=clf.classes_\n",
    "            )\n",
    "        else:\n",
    "            pred = fit.predict(X1)\n",
    "            scr0.loc[i] = accuracy_score(y1, pred, sample_weight=w1.values)\n",
    "\n",
    "        for j in X.columns:\n",
    "            X1_ = X1.copy(deep=True)\n",
    "            np.random.shuffle(X1_[j].values)  # permutation of a single column\n",
    "            if scoring == \"neg_log_loss\":\n",
    "                prob = fit.predict_proba(X1_)\n",
    "                scr1.loc[i, j] = -log_loss(\n",
    "                    y1, prob, sample_weight=w1.values, labels=clf.classes_\n",
    "                )\n",
    "            else:\n",
    "                pred = fit.predict(X1_)\n",
    "                scr1.loc[i, j] = accuracy_score(y1, pred, sample_weight=w1.values)\n",
    "\n",
    "    imp = (-scr1).add(scr0, axis=0)\n",
    "    if scoring == \"neg_log_loss\":\n",
    "        imp = imp / -scr1\n",
    "    else:\n",
    "        imp = imp / (1.0 - scr1)\n",
    "\n",
    "    imp = pd.concat(\n",
    "        {\"mean\": imp.mean(), \"std\": imp.std() * imp.shape[0] ** -0.5}, axis=1\n",
    "    )\n",
    "    return imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
