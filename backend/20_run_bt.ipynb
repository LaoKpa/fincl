{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp run_bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import random\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from mlbt.load_data import (\n",
    "    get_symbols,\n",
    "    load_and_sample_bars,\n",
    "    load_bars,\n",
    "    save_bars,\n",
    "    load_events_b,\n",
    "    save_events_b,\n",
    "    load_feats,\n",
    "    save_feats,\n",
    "    load_imp,\n",
    "    save_imp,\n",
    "    load_payload,\n",
    "    save_payload,\n",
    ")\n",
    "from mlbt.filters import cusum\n",
    "from mlbt.multiprocess import mp_pandas_obj\n",
    "from mlbt.utils import get_daily_vol, NumpyEncoder\n",
    "from mlbt.get_bins import get_bins, drop_labels\n",
    "from mlbt.alpha import ma_alpha, bb_alpha\n",
    "from mlbt.binarize import triple_barrier_method, fixed_horizon\n",
    "from mlbt.feature_eng import engineer_features, define_features\n",
    "from mlbt.reporting import get_reports\n",
    "from mlbt.models import get_model\n",
    "from mlbt.feature_importance import feat_importance\n",
    "\n",
    "FORMAT = \"%(asctime)-15s %(message)s\"\n",
    "logging.basicConfig(format=FORMAT, level=logging.DEBUG)\n",
    "\n",
    "SYMBOL_GROUPS = {\n",
    "    \"agriculture\": \"Agriculture\",\n",
    "    \"currency\": \"Currency\",\n",
    "    \"energy\": \"Energy\",\n",
    "    \"equity_index\": \"Equity Index\",\n",
    "    \"interest_rate\": \"Interest Rate\",\n",
    "    \"metals\": \"Metals\",\n",
    "}\n",
    "\n",
    "\n",
    "def downsample(bars, type_, daily_vol):\n",
    "    if type_ == \"cusum\":\n",
    "        return cusum(bars[\"Close\"], daily_vol.mean())\n",
    "\n",
    "    return bars.index\n",
    "\n",
    "\n",
    "def alpha(bars, events, type_, params):\n",
    "    if type_ == \"none\":\n",
    "        return events\n",
    "    elif type_ == \"ma-cross\":\n",
    "        signal = ma_alpha(bars, *params)\n",
    "    elif type_ == \"bbands-mr\":\n",
    "        signal = bb_alpha(bars, *params, True)\n",
    "    elif type_ == \"bbands-tf\":\n",
    "        signal = bb_alpha(bars, *params, False)\n",
    "\n",
    "    events[\"side\"] = signal\n",
    "\n",
    "    assert set(events[\"side\"].dropna()) == set([1, -1]), set(events[\"side\"].dropna())\n",
    "    return events\n",
    "\n",
    "\n",
    "def make_bins(bars, events):\n",
    "    bins = get_bins(events, bars[\"Close\"])\n",
    "    bins = drop_labels(bins)\n",
    "    return bins\n",
    "\n",
    "\n",
    "def join_importances(deck):\n",
    "    \"\"\"Join the feature importances computed parallelized & per-symbol into one dataframe\"\"\"\n",
    "    dfs = [x['imp'] for x in deck.values()]\n",
    "    mean = pd.concat([x[\"mean\"] for x in dfs], axis=1).mean(axis=1)\n",
    "    std = pd.concat([x[\"std\"] for x in dfs], axis=1).std(axis=1) * len(dfs) ** -0.5\n",
    "\n",
    "    return pd.DataFrame({\"mean\": mean, \"std\": std})\n",
    "\n",
    "\n",
    "def pick_good_features(imp_all, columns, method):\n",
    "    \"\"\"Pick features that help our classifier's predictive abilities\"\"\"\n",
    "    imp_d = imp_all[\"mean\"].to_dict()\n",
    "    cutoff = 0 if method == \"MDA\" else imp_all[\"mean\"].mean()\n",
    "    cols = [col for col in columns if imp_d[col] > cutoff]\n",
    "    logging.info(f\"Picked {len(cols)}/{len(columns)} important features: {cols}\")\n",
    "\n",
    "    return cols\n",
    "\n",
    "\n",
    "def combine_symbol_decks(deck):\n",
    "    \"\"\"\n",
    "    Join events, features and bins that have been computed on a per-symbol level into one\n",
    "    grand data-frame. To note: In order to in the future still be able to differentiate which row belongs\n",
    "    to which symbol we embed the symbols position in our symbols table into the microseconds of the index.\n",
    "    This is not in any way good code, but it allows us to still have a unique & sortable index without\n",
    "    resorting to multi-indices or the like. This is predicated on the fact that we know we only sample from\n",
    "    1-minute bars. A.k.a Poor Man's Multi-Index\n",
    "    \"\"\"\n",
    "    e_x_ys = {}\n",
    "    for i, (symbol, symbol_deck) in enumerate(deck.items()):\n",
    "        e_x_y = symbol_deck['e_x_y']\n",
    "        events_train, X_train, y_train, events_test, X_test, y_test = e_x_y\n",
    "        y_train = y_train.to_frame()\n",
    "        y_test = y_test.to_frame()\n",
    "\n",
    "        # every row for every symbol has a unique datetime index and is sortable\n",
    "        for df in [events_train, X_train, y_train, events_test, X_test, y_test]:\n",
    "            df.index += pd.Timedelta(i, \"us\")\n",
    "\n",
    "        events_train[\"t1\"] += pd.Timedelta(i, \"us\")\n",
    "        events_test[\"t1\"] += pd.Timedelta(i, \"us\")\n",
    "\n",
    "        e_x_ys[symbol] = (events_train, X_train, y_train, events_test, X_test, y_test)\n",
    "\n",
    "    grand_frames = []\n",
    "    for list_of_dfs in zip(*e_x_ys.values()):\n",
    "        grand_frame = pd.concat(list_of_dfs)\n",
    "        grand_frame = grand_frame.sort_index()\n",
    "        grand_frames.append(grand_frame)\n",
    "\n",
    "    return grand_frames\n",
    "\n",
    "\n",
    "def train_test_split(bars, events, feats, bins, start_date=None, end_date=None):\n",
    "    \"\"\"\n",
    "    Exclude rows from our engineered features which haven't completed the warmup for all feature columns\n",
    "    and split the set 50/50 into train & test set\n",
    "    \"\"\"\n",
    "    X_all = feats\n",
    "    y_all = bins[\"bin\"]\n",
    "    y_all = bins\n",
    "\n",
    "    # Drop all rows where we don't have a complete set of features\n",
    "    merged = pd.merge(X_all, y_all, left_index=True, right_index=True).dropna()\n",
    "    merged = merged.truncate(before=start_date, after=end_date)\n",
    "\n",
    "    X_all = merged.drop(columns=bins.columns)\n",
    "    y_all = merged[\"bin\"]\n",
    "\n",
    "    events_all = events[events.index.isin(X_all.index)]\n",
    "    # Store all-kinds-of-information in events for later PnL calculations\n",
    "    events_all[bins.columns] = merged[bins.columns]\n",
    "    events_all[\"close_p\"] = bars[\"Close\"][bars.index.isin(events.index)]\n",
    "\n",
    "    cut = X_all.shape[0] // 2\n",
    "    events_train, events_test = events_all.iloc[:cut], events_all.iloc[cut:]\n",
    "    X_train, X_test = X_all.iloc[:cut], X_all.iloc[cut:]\n",
    "    y_train, y_test = y_all.iloc[:cut], y_all.iloc[cut:]\n",
    "\n",
    "    logging.info(f\"bars {bars.shape}, X_all {X_all.shape}, X_train {X_train.shape}\")\n",
    "\n",
    "    return (events_train, X_train, y_train, events_test, X_test, y_test)\n",
    "\n",
    "\n",
    "def binarize(bars, t_events, type_, binarize_params, daily_vol, num_threads):\n",
    "    \"\"\"\n",
    "    Binarize the rows, i.e. for every row determine a forward returns window which\n",
    "    is then used to calculate that row's label\n",
    "    \"\"\"\n",
    "    if type_ == \"fixed_horizon\":\n",
    "        return fixed_horizon(t_events, binarize_params)\n",
    "    elif type_ == \"triple_barrier_method\":\n",
    "        return triple_barrier_method(\n",
    "            bars, t_events, binarize_params, daily_vol, num_threads\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "def prepare_payload(config, symbols, imp_all, reports):\n",
    "    \"\"\"Prepare payload for serialization\"\"\"\n",
    "    config[\"start_date\"] = config[\"start_date\"].isoformat()\n",
    "    config[\"end_date\"] = config[\"end_date\"].isoformat()\n",
    "\n",
    "    return {\n",
    "        \"symbols\": symbols,\n",
    "        \"feature_importance\": imp_all.to_dict(),\n",
    "        \"config\": config,\n",
    "        **reports,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_symbols_list(config):\n",
    "    if config[\"symbols\"]:\n",
    "        symbols = config[\"symbols\"]\n",
    "    else:\n",
    "        symbols = get_symbols(config[\"symbol_groups\"])\n",
    "\n",
    "    symbols = [x for x in symbols if x not in IGNORE_SYMBOLS]\n",
    "    return symbols\n",
    "    \n",
    "\n",
    "def abort_early(config):\n",
    "    if config[\"check_completed\"]:\n",
    "        symbols = get_symbols_list(config)\n",
    "        payload = load_payload(symbols, config)\n",
    "        if payload is not None:\n",
    "            logging.info(\"We have the payload, not recomputing\")\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "\n",
    "def parse_config(data):\n",
    "    \"\"\"Turn the input parameters into the config object which is used as configuration throughout the project\"\"\"\n",
    "    alpha, *alpha_params = data[\"alpha\"].split(\"_\")\n",
    "\n",
    "    alpha_params = [float(x) if \".\" in x else int(x) for x in alpha_params]\n",
    "\n",
    "    default_binarize_params = {\"triple_barrier_method\": [1, 1, 1], \"fixed_horizon\": 100}\n",
    "    binarize_params = data.get(\"binarize_params\") or default_binarize_params[data[\"binarize\"]]\n",
    "\n",
    "    return {\n",
    "        \"start_date\": data.get(\"start_date\", date(2000, 1, 1)),\n",
    "        \"end_date\": data.get(\"end_date\", date(2020, 1, 1)),\n",
    "        \"vol_estimate\": 100,\n",
    "        \"downsampling\": \"cusum\",\n",
    "        \"symbols\": data.get(\"symbols\"),\n",
    "        \"symbol_groups\": data.get(\"symbol_groups\"),\n",
    "        \"test_procedure\": \"walk_forward\",\n",
    "        \"classifier\": data[\"classifier\"],\n",
    "        \"bar_type\": data[\"bar_type\"],\n",
    "        \"bar_size\": None,\n",
    "        \"binarize\": data[\"binarize\"],\n",
    "        \"binarize_params\": binarize_params,\n",
    "        \"alpha\": alpha,\n",
    "        \"alpha_params\": alpha_params,\n",
    "        \"feature_imp_only\": data.get(\"feature_imp_only\", False),\n",
    "        \"reuse_hypers\": data.get(\"reuse_hypers\", True),\n",
    "        \"hypers_n_iter\": data.get(\"hypers_n_iter\", 25),\n",
    "        \"save_to_disk\": data.get(\"save_to_disk\", True),\n",
    "        \"optimize_hypers\": data.get(\"optimize_hypers\", True),\n",
    "        \"feat_imp_method\": data.get(\"feat_imp_method\", \"MDA\"),\n",
    "        \"num_threads\": data.get(\"num_threads\", 32),\n",
    "        \"n_jobs\": data.get(\"n_jobs\", 4),\n",
    "        \"check_completed\": data.get(\"check_completed\", False),\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "# TODO: Figure out why Lean Hogs break our code\n",
    "IGNORE_SYMBOLS = [\"@LH#C\"]\n",
    "\n",
    "\n",
    "def load_sample_and_binarize(config):\n",
    "    \"\"\"\n",
    "    Load our bars, chunk them into dollar bars aiming to have 50 bars per day per symbol for the year 2019.\n",
    "    These bars are then CUSUM downsampled and binarized before being saved for later runs.\n",
    "    \"\"\"\n",
    "    symbols = get_symbols_list(config)\n",
    "\n",
    "    logging.info(f\"Symbols: {symbols}\")\n",
    "    deck = {}\n",
    "    for symbol in symbols:\n",
    "        bars = load_bars(symbol, config)\n",
    "        if bars is None:\n",
    "            bars, bar_size = load_and_sample_bars(symbol, config)\n",
    "            if config[\"save_to_disk\"]:\n",
    "                save_bars(symbol, config, bars)\n",
    "\n",
    "        events_b = load_events_b(symbol, config)\n",
    "        if events_b is None:\n",
    "            daily_vol = get_daily_vol(bars[\"Close\"], config[\"vol_estimate\"])\n",
    "            t_events = downsample(bars, config[\"downsampling\"], daily_vol)\n",
    "            logging.info(f\"Downsampled from {len(bars)} to {len(t_events)}\")\n",
    "\n",
    "            logging.debug(f\"Binarize {config['binarize']}\")\n",
    "            events_b = binarize(\n",
    "                bars,\n",
    "                t_events,\n",
    "                config[\"binarize\"],\n",
    "                config[\"binarize_params\"],\n",
    "                daily_vol,\n",
    "                config[\"num_threads\"],\n",
    "            )\n",
    "\n",
    "            if config[\"save_to_disk\"]:\n",
    "                save_events_b(symbol, config, events_b)\n",
    "\n",
    "        deck[symbol] = {'bars': bars, 'events_b': events_b}\n",
    "\n",
    "    return deck\n",
    "\n",
    "\n",
    "def run_feature_engineering(config, deck):\n",
    "    \"\"\"Load already-engineered features or engineer if we can't\"\"\"\n",
    "    for symbol, symbol_deck in deck.items():\n",
    "        logging.debug(symbol)\n",
    "        bars = symbol_deck['bars']\n",
    "\n",
    "        feats = load_feats(symbol, config)\n",
    "        if feats is None:\n",
    "            feats = engineer_features(bars, config[\"features\"])\n",
    "            if config[\"save_to_disk\"]:\n",
    "                save_feats(symbol, config, feats)\n",
    "        deck[symbol]['feats'] = feats\n",
    "    return deck\n",
    "\n",
    "\n",
    "def prepare_alpha_bins_feature_imps(config, deck):\n",
    "    for symbol, symbol_deck in deck.items():\n",
    "        bars, events_b, feats = symbol_deck['bars'], symbol_deck['events_b'], symbol_deck['feats']\n",
    "        events = alpha(\n",
    "            bars, events_b, config[\"alpha\"], config[\"alpha_params\"]\n",
    "        )\n",
    "\n",
    "        bins = make_bins(bars, events)\n",
    "\n",
    "        e_x_y = train_test_split(\n",
    "            bars,\n",
    "            events,\n",
    "            feats,\n",
    "            bins,\n",
    "            config[\"start_date\"],\n",
    "            config[\"end_date\"],\n",
    "        )\n",
    "        events_train, X_train, y_train, events_test, X_test, y_test = e_x_y\n",
    "\n",
    "        imp = load_imp(symbol, config)\n",
    "        if imp is None:\n",
    "            imp = feat_importance(\n",
    "                events_train,\n",
    "                X_train,\n",
    "                y_train,\n",
    "                cv=5,\n",
    "                method=config[\"feat_imp_method\"],\n",
    "                num_threads=config[\"num_threads\"],\n",
    "                \n",
    "            )\n",
    "            if config[\"save_to_disk\"]:\n",
    "                save_imp(symbol, config, imp)\n",
    "\n",
    "        deck[symbol] = {'imp': imp, 'e_x_y': e_x_y}\n",
    "\n",
    "    return deck\n",
    "\n",
    "\n",
    "def run_ml_pipe(config, deck):\n",
    "    \"\"\"\n",
    "    Run the large chunk of our ML pipeline, which includes calculating the primary (and secondary) models,\n",
    "    splitting our data into train/test sets, calulating feature importances, hyper-parameter optimization,\n",
    "    model fitting and evaluation and generation of final reports which are later user for PnL simulations.\n",
    "    \"\"\"\n",
    "    if config[\"feature_imp_only\"]:\n",
    "        return\n",
    "\n",
    "    symbols = list(deck.keys())\n",
    "    for symbol, symbol_deck in deck.items():\n",
    "        logging.debug(f\"{symbol} {[x.shape for x in symbol_deck['e_x_y']]}\")\n",
    "    \n",
    "    grand_frames = combine_symbol_decks(deck)\n",
    "    events_train, X_train, y_train, events_test, X_test, y_test = grand_frames\n",
    "    y_train, y_test = y_train[\"bin\"], y_test[\"bin\"]\n",
    "\n",
    "    # Important feats\n",
    "    imp_all = join_importances(deck)\n",
    "    cols = pick_good_features(imp_all, X_train.columns, config[\"feat_imp_method\"])\n",
    "    X_train, X_test = X_train[cols], X_test[cols]\n",
    "    del deck\n",
    "\n",
    "    hyper_params = None\n",
    "    # Try loading the payload so we can re-use hyper parameters from previous run\n",
    "    payload = load_payload(symbols, config)\n",
    "    if payload is not None:\n",
    "        if config[\"reuse_hypers\"]:\n",
    "            report = payload[\"secondary\"] or payload[\"primary\"]\n",
    "            hyper_params = report[\"hyper_params\"]\n",
    "            logging.info(f\"Loaded hypers {hyper_params}\")\n",
    "\n",
    "    model, hyper_params = get_model(\n",
    "        events_train,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        config[\"classifier\"],\n",
    "        config[\"optimize_hypers\"],\n",
    "        config[\"hypers_n_iter\"],\n",
    "        config[\"num_threads\"],\n",
    "        config[\"n_jobs\"],\n",
    "        hyper_params,\n",
    "    )\n",
    "\n",
    "    reports = get_reports(\n",
    "        model,\n",
    "        events_test,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        config[\"test_procedure\"],\n",
    "        config[\"alpha\"] != \"none\",\n",
    "        hyper_params,\n",
    "    )\n",
    "\n",
    "    saved_path = \"\"\n",
    "    if config[\"save_to_disk\"]:\n",
    "        payload = prepare_payload(config, symbols, imp_all, reports)\n",
    "        saved_path = save_payload(symbols, config, payload)\n",
    "\n",
    "    return saved_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def run_bt(**data):\n",
    "    config = parse_config(data)\n",
    "    config['features'] = define_features()\n",
    "    logging.info(f\"config: {config}\")\n",
    "    \n",
    "    if abort_early(config):\n",
    "        return ''\n",
    "\n",
    "    # We store every symbol's data and computations in a central \"deck\" dictionary\n",
    "    deck = load_sample_and_binarize(config)\n",
    "    deck = run_feature_engineering(config, deck)\n",
    "    deck = prepare_alpha_bins_feature_imps(config, deck)\n",
    "    payload_path = run_ml_pipe(config, deck)\n",
    "    return payload_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    'start_date': date(2019, 1, 1),\n",
    "    'end_date': date(2021, 1, 1),\n",
    "    # \"symbol_groups\": [\"equity_index\"],\n",
    "    # \"symbol_groups\": list(SYMBOL_GROUPS.keys()),\n",
    "    \"symbols\": [\"@ES#C\", \"@NQ#C\"],\n",
    "    \"bar_type\": \"dollar\",\n",
    "    \"binarize\": \"fixed_horizon\",\n",
    "    \"binarize_params\": 10,\n",
    "    # \"alpha\": \"bbands-mr_500_1.5\",\n",
    "    # \"alpha\": \"ma-cross_51_500\",\n",
    "    \"alpha\": \"none\",\n",
    "    \"classifier\": \"random_forest\",\n",
    "    # \"classifier\": \"xgboost\",\n",
    "    \"classifier\": \"lgbm\",\n",
    "    # \"classifier\": \"dummy\",\n",
    "    # \"classifier\": \"knn\",\n",
    "    # 'hypers_n_iter': 5,\n",
    "    # 'optimize_hypers': False,\n",
    "    # 'feature_imp_only': True,\n",
    "    # 'reuse_hypers': False,\n",
    "    'num_threads': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result_path = run_bt(**conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
