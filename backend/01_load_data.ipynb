{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/doda/anaconda3/envs/metal/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.ensemble.bagging module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/doda/anaconda3/envs/metal/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.ensemble.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "from path import Path\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from mlfinlab.data_structures import get_dollar_bars, get_tick_bars, get_time_bars, get_volume_bars\n",
    "\n",
    "from mlbt.utils import NumpyEncoder\n",
    "\n",
    "# You'll likely have to change these if you're intending to run the code yourself\n",
    "# TODO: Factor out into settings.py file\n",
    "DATA_DIR = Path(\"~/Dropbox/algotrading/data\").expanduser()\n",
    "F_PAYLOAD_DIR = Path(\"~/pr/fincl/frontend/public/payloads\").expanduser()\n",
    "DAILY_DATA_DIR = DATA_DIR / \"daily\"\n",
    "\n",
    "SYMBOLS_CSV = pd.read_csv(DATA_DIR / \"symbols.csv\", index_col=\"iqsymbol\")\n",
    "\n",
    "LENGTH_RANKING = ['@SP#C','@W#C','@BO#C','@QM#C','@QG#C','EZ#C','@KC#C','@C#C','XG#C','@S#C','EX#C','@SB#C','@OJ#C','@NKD#C','@EMD#C','BD#C','LF#C','QNG#C','QCL#C','@HE#C','@CC#C','@CT#C','@LE#C','@ES#C','@TY#C','QSI#C','QPL#C','@FV#C','@O#C','LG#C','@SM#C','@GF#C','QGC#C','@YM#C','@TU#C','QPA#C','@UB#C','@NQ#C','@ED#C','GAS#C','@MME#C','QHG#C','QHO#C','@BP#C','@CD#C','@RP#C','@NE#C','@AD#C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def get_symbols(symbol_groups):\n",
    "    lists = {\"us_index\": [\"@NQ#C\", \"@ES#C\", \"@YM#C\"]}\n",
    "    if len(symbol_groups) == 1 and symbol_groups[0] in lists:\n",
    "        return lists[symbol_groups[0]]\n",
    "\n",
    "    sectors = {\n",
    "        \"agriculture\": \"Agriculture\",\n",
    "        \"currency\": \"Currency\",\n",
    "        \"energy\": \"Energy\",\n",
    "        \"equity_index\": \"Equity Index\",\n",
    "        \"interest_rate\": \"Interest Rate\",\n",
    "        \"metals\": \"Metals\",\n",
    "    }\n",
    "    symbol_groups = [sectors[x] for x in symbol_groups]\n",
    "    picked = SYMBOLS_CSV[SYMBOLS_CSV[\"Sector\"].isin(symbol_groups)]\n",
    "\n",
    "    ignore = [\"@LH#C\"]\n",
    "    return [x for x in picked.index.values if x not in ignore]\n",
    "\n",
    "\n",
    "def load_contract(contract_name, directory):\n",
    "    series = pd.read_csv(\n",
    "        DATA_DIR / directory / \"{}.csv\".format(contract_name), index_col=0\n",
    "    )\n",
    "    series = series[::-1]\n",
    "    if directory == \"minutely\":\n",
    "        series[\"Time\"] = series[\"date\"] + \" \" + series[\"time\"]\n",
    "        series = series.set_index(\n",
    "            pd.to_datetime(series[\"Time\"], format=\"%Y-%m-%d 0 days %H:%M:00.000000000\")\n",
    "        )\n",
    "    else:\n",
    "        series[\"Time\"] = series[\"date\"]\n",
    "        series = series.set_index(pd.to_datetime(series[\"Time\"], format=\"%Y-%m-%d\"))\n",
    "\n",
    "    series = series[[\"open_p\", \"close_p\", \"prd_vlm\", \"Time\"]]\n",
    "    series = series.rename(\n",
    "        columns={\"close_p\": \"Close\", \"open_p\": \"Open\", \"prd_vlm\": \"Volume\"}\n",
    "    )\n",
    "    series[\"Instrument\"] = contract_name\n",
    "    return series\n",
    "\n",
    "\n",
    "def load_contracts(symbol, directory=\"minutely\", start_date=None, end_date=None):\n",
    "    contract_names = [\n",
    "        x.basename().namebase\n",
    "        for x in (DATA_DIR / directory).files(\"*{}*\".format(symbol))\n",
    "    ]\n",
    "    loaded = [load_contract(x, directory) for x in contract_names]\n",
    "    loaded = list(sorted(loaded, key=lambda x: x.index[-1]))\n",
    "    first = loaded[0]\n",
    "    # cut out from later contracts what former contracts already have\n",
    "    zipped = zip(loaded, loaded[1:])\n",
    "    cut_contracts = [\n",
    "        latter.truncate(before=former.index[-1] + pd.Timedelta(minutes=1))\n",
    "        for former, latter in zipped\n",
    "    ]\n",
    "\n",
    "    concatted = pd.concat([first] + cut_contracts)\n",
    "    return concatted.truncate(before=start_date, after=end_date)\n",
    "\n",
    "\n",
    "def load_all_cont_contracts():\n",
    "    all_continuous_contracts = DAILY_DATA_DIR.files(\"*#C*\")\n",
    "    all_continuous_contracts = [x.basename().namebase for x in all_continuous_contracts]\n",
    "    return {name: load_contract(name, \"daily\") for name in all_continuous_contracts}\n",
    "\n",
    "\n",
    "def get_data(symbol, frequency, start_date, end_date):\n",
    "    # Include up to 1 year prior for feature engineering\n",
    "    # we blindly assume no code wants a longer warm-up period than that\n",
    "    return load_contracts(\n",
    "        symbol,\n",
    "        frequency,\n",
    "        start_date - relativedelta(years=1) if start_date else None,\n",
    "        end_date,\n",
    "    )\n",
    "\n",
    "def process_bars(bars, size, type_, resolution=\"MIN\"):\n",
    "    # Renaming our bar columns & format for mlfinlab for processing and then back into our original format\n",
    "    # OHL from 1-min bars are ignored\n",
    "    fun = {\n",
    "        \"time\": get_time_bars,\n",
    "        \"tick\": get_tick_bars,\n",
    "        \"volume\": get_volume_bars,\n",
    "        \"dollar\": get_dollar_bars,\n",
    "    }[type_]\n",
    "\n",
    "    bars = bars[['Close', 'Volume']].reset_index()\n",
    "    bars.columns = ['date_time', 'close', 'volume']\n",
    "    if type_ == \"time\":\n",
    "        s_bars = fun(bars, resolution=resolution, num_units=size)\n",
    "    else:\n",
    "        s_bars = fun(bars, threshold=size)\n",
    "    bars = s_bars[['date_time', 'open', 'high', 'low', 'close', 'volume', 'cum_dollar_value', 'cum_ticks', 'cum_buy_volume']]\n",
    "    bars.columns = ['Time', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dollar Volume', 'Num Ticks', 'Buy Volume']\n",
    "    if type_ == \"time\":\n",
    "        bars = bars.set_index(s_bars['date_time'].apply(datetime.fromtimestamp).values)\n",
    "    else:\n",
    "        bars = bars.set_index('Time', drop=False)\n",
    "    return bars\n",
    "\n",
    "\n",
    "def load_and_sample_bars(symbol, start_date, end_date, type_, size=None):\n",
    "    bars = get_data(symbol, \"minutely\", start_date, end_date)\n",
    "    bars[\"Dollar Volume\"] = bars[\"Volume\"] * bars[\"Close\"]\n",
    "\n",
    "    if size is None:\n",
    "        size = determine_bar_size(bars, type_)\n",
    "\n",
    "    return process_bars(bars, size, type_), size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def determine_bar_size(bars, bar_type):\n",
    "    # Return bar size to have approx. 25 bars per day for the year 2019\n",
    "    col = {\"dollar\": \"Dollar Volume\", \"volume\": \"Volume\"}[bar_type]\n",
    "    bar_size = bars[bars.index.year == 2019][col].sum() / 252 / 25\n",
    "    return bar_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "def safe_feat_name(feat_c, safe_for_fs=True):\n",
    "    \"\"\"Turn our {feature:dict} into a pythonic function_invocation(like=string) for display & file-path purposes\"\"\"\n",
    "    feat_c = feat_c.copy()\n",
    "    for k,v in feat_c.items():\n",
    "        if isinstance(v, dict):\n",
    "            feat_c[k] = safe_feat_name(v, safe_for_fs)\n",
    "\n",
    "    if safe_for_fs:\n",
    "        name = feat_c.pop(\"name\")\n",
    "        dumped = json.dumps(\n",
    "            feat_c,\n",
    "            sort_keys=True,\n",
    "            separators=(',', '=')\n",
    "        ).replace('\"', '').replace('{', '(').replace('}', ')')\n",
    "        return f\"{name}{dumped}\"\n",
    "    else:\n",
    "        return json.dumps(\n",
    "            feat_c,\n",
    "            sort_keys=True,\n",
    "        )\n",
    "\n",
    "def load_hdf(path):\n",
    "    try:\n",
    "        if path.exists() and path.size:\n",
    "            return pd.read_hdf(path, 'table')\n",
    "    except:\n",
    "        logging.exception(\"could not load_hdf\")\n",
    "        # Does this fix our weird error?\n",
    "        with open(path) as f:\n",
    "            pass\n",
    "\n",
    "\n",
    "def save_hdf(obj, path):\n",
    "    obj.to_hdf(path, 'table')\n",
    "    return path\n",
    "\n",
    "\n",
    "def bars_path(symbol, c):\n",
    "    return DATA_DIR / c['bar_type'] / f\"{symbol}_bars.h5\"\n",
    "\n",
    "\n",
    "def events_b_path(symbol, c):\n",
    "    return DATA_DIR / c['bar_type'] / f\"{symbol}_events_{c['vol_estimate']}_{c['binarize']}_{c['binarize_params']}_{c['downsampling']}.h5\"\n",
    "\n",
    "\n",
    "def feats_path(symbol, c):\n",
    "    feat_names = '-'.join(sorted(set(x.split('_')[0] for x in c['features'])))\n",
    "    return DATA_DIR / c['bar_type'] / f\"{symbol}_feats_{feat_names}.h5\"\n",
    "\n",
    "\n",
    "def feat_path(c, feat_c):\n",
    "    # Make a compact, unique path for this feature config\n",
    "    basename = safe_feat_name(feat_c)\n",
    "    return DATA_DIR / 'features' / c['bar_type'] / f\"{basename}.h5\"\n",
    "\n",
    "def find_params(d, s=None):\n",
    "    s = s or set()\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, dict):\n",
    "            return find_params(v, s)\n",
    "        else:\n",
    "            s.add(str(v))\n",
    "        s.add(str(k))\n",
    "    return s\n",
    "\n",
    "def imp_path(symbol, c):\n",
    "    feat_names = set.union(*[find_params(feat) for feat in c['features']])\n",
    "    feat_names = ''.join([x[:2] for x in sorted(feat_names - {'name', 'window', 'stdev', 'lag'})])\n",
    "    return DATA_DIR / c['bar_type'] / f\"{symbol}_fimp_{c['binarize']}_{c['binarize_params']}_{c['alpha']}_{c['alpha_params']}_{feat_names}_{c['feat_imp_method']}.h5\"\n",
    "\n",
    "\n",
    "def payload_path(symbols, c):\n",
    "    symbols_s = '-'.join(c['symbol_groups'] or c['symbols'])\n",
    "    return DATA_DIR / 'payloads' / f\"payload_{symbols_s}_{c['bar_type']}_{c['binarize']}_{c['binarize_params']}_{c['alpha']}_{c['alpha_params']}_{c['classifier']}.json\"\n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "def load_bars(symbol, config):\n",
    "    if config[\"load_from_disk\"]:\n",
    "        path = bars_path(symbol, config)\n",
    "        return load_hdf(path)\n",
    "\n",
    "\n",
    "def save_bars(symbol, config, bars):\n",
    "    if config[\"save_to_disk\"]:\n",
    "        path = bars_path(symbol, config)\n",
    "        return save_hdf(bars, path)\n",
    "\n",
    "\n",
    "def load_events_b(symbol, config):\n",
    "    if config[\"load_from_disk\"]:\n",
    "        path = events_b_path(symbol, config)\n",
    "        return load_hdf(path)\n",
    "\n",
    "\n",
    "def save_events_b(symbol, config, events_b):\n",
    "    if config[\"save_to_disk\"]:\n",
    "        path = events_b_path(symbol, config)\n",
    "        return save_hdf(events_b, path)\n",
    "\n",
    "\n",
    "def load_feat(config, feat_config):\n",
    "    if config[\"load_from_disk\"]:\n",
    "        path = feat_path(config, feat_config)\n",
    "        return load_hdf(path)\n",
    "\n",
    "\n",
    "def save_feat(config, feat_config, feat):\n",
    "    if config[\"save_to_disk\"]:\n",
    "        path = feat_path(config, feat_config)\n",
    "        return save_hdf(feat, path)\n",
    "\n",
    "\n",
    "def load_imp(symbol, config):\n",
    "    if config[\"load_from_disk\"]:\n",
    "        path = imp_path(symbol, config)\n",
    "        return load_hdf(path)\n",
    "\n",
    "\n",
    "def save_imp(symbol, config, imp):\n",
    "    if config[\"save_to_disk\"]:\n",
    "        path = imp_path(symbol, config)\n",
    "        return save_hdf(imp, path)\n",
    "\n",
    "\n",
    "def load_payload(symbols, config):\n",
    "    if config[\"load_from_disk\"]:\n",
    "        path = payload_path(symbols, config)\n",
    "        try:\n",
    "            if path.exists() and path.size:\n",
    "                with open(path) as f:\n",
    "                    return json.load(f)\n",
    "        except:\n",
    "            logging.error(f\"corrupted payload: {path}\")\n",
    "\n",
    "\n",
    "def save_payload(symbols, config, payload):\n",
    "    if config[\"save_to_disk\"]:\n",
    "        path = payload_path(symbols, config)\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(payload, f, cls=NumpyEncoder)\n",
    "        return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'auto(lag=10,symbol=stdev(symbol=ffd(d=0.5),window=250),window=250)'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat1 = {'name':'a', 'b':2, 'c':3, 'd':{'name': 'dede', 'bb':33}}\n",
    "feat2=   {'name': 'auto',\n",
    "    'window': 250,\n",
    "    'lag': 10,\n",
    "    'symbol': {'name': 'stdev',\n",
    "     'window': 250,\n",
    "     'symbol': {'name': 'ffd', 'd': 0.5}}}\n",
    "\n",
    "safe_feat_name(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('C:\\\\Users\\\\doda/Dropbox/algotrading/data\\\\derp\\\\arst_fimp_yes_yes_no_no_0.5-10-2-250-3-33-a-auto-b-bb-c-d-dede-ffd_ya.h5')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_path('arst', {'features': [feat1, feat2], 'bar_type': 'derp', 'binarize': 'yes', 'binarize_params': 'yes', 'alpha': 'no', 'alpha_params': 'no', 'feat_imp_method': 'ya'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
